---
---

@article{chinchure2025tibet,
  title={TIBET: Identifying and evaluating biases in text-to-image generative models},
  author={Chinchure*, Aditya and Shukla*, Pushkar and Bhatt, Gaurav and Salij, Kiri and Hosanagar, Kartik and Sigal, Leonid and Turk, Matthew},
  booktitle={European Conference on Computer Vision},
  pages={429--446},
  year={2024},
  organization={Springer},
  abbr={ECCV2024},
  abstract={Text-to-Image (TTI) generative models have shown great progress in the past few years in terms of their ability to generate complex and high-quality imagery. At the same time, these models have been shown to suffer from harmful biases, including exaggerated societal biases (e.g., gender, ethnicity), as well as incidental correlations that limit such a model's ability to generate more diverse imagery. In this paper, we propose a general approach to study and quantify a broad spectrum of biases, for any TTI model and for any prompt, using counterfactual reasoning. Unlike other works that evaluate generated images on a predefined set of bias axes, our approach automatically identifies potential biases that might be relevant to the given prompt, and measures those biases. In addition, we complement quantitative scores with post-hoc explanations in terms of semantic concepts in the images generated. We show that our method is uniquely capable of explaining complex multi-dimensional biases through semantic concepts, as well as the intersectionality between different biases for any given prompt. We perform extensive user studies to illustrate that the results of our method and analysis are consistent with human judgements.},
  website={https://tibet-ai.github.io},
  selected={true},
}

@article{shukla2024utilizingadversarialexamplesbias,
  title={Utilizing Adversarial Examples for Bias Mitigation and Accuracy Enhancement}, 
  author={Pushkar Shukla and Dhruv Srikanth and Lee Cohen and Matthew Turk},
  year={2024},
  eprint={2404.11819},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2404.11819}, 
  abbr={Preprint},
  abstract={We propose a novel approach to mitigate biases in computer vision models by utilizing counterfactual generation and fine-tuning. While counterfactuals have been used to analyze and address biases in DNN models, the counterfactuals themselves are often generated from biased generative models, which can introduce additional biases or spurious correlations. To address this issue, we propose using adversarial images, that is images that deceive a deep neural network but not humans, as counterfactuals for fair model training. Our approach leverages a curriculum learning framework combined with a fine-grained adversarial loss to fine-tune the model using adversarial examples. By incorporating adversarial images into the training data, we aim to prevent biases from propagating through the pipeline. We validate our approach through both qualitative and quantitative assessments, demonstrating improved bias mitigation and accuracy compared to existing methods. Qualitatively, our results indicate that post-training, the decisions made by the model are less dependent on the sensitive attribute and our model better disentangles the relationship between sensitive attributes and classification variables.},
}

@InProceedings{Shukla_2023_CVPR,
    author    = {Shukla, Pushkar and Bharati, Sushil and Turk, Matthew},
    title     = {CAVLI - Using Image Associations To Produce Local Concept-Based Explanations},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2023},
    pages     = {3750-3755},
    url={https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/html/Shukla_CAVLI_-_Using_Image_Associations_To_Produce_Local_Concept-Based_Explanations_CVPRW_2023_paper.html}, 
    abbr={CVPR2023 W},
    abstract={While explainability is becoming increasingly crucial in computer vision and machine learning, producing explanations that are able to link decisions made by deep neural networks to concepts that are easily understood by humans still remains a challenge. To address this challenge, we propose a framework that produces local concept based explanations for the classification decisions made by a deep neural network. Our framework is based on the intuition that if there is a high overlap between the regions of the image that the model associates the most with the concept and the regions of the image that are useful for decision-making then the decision is highly dependent on the concept. Our proposed CAVLI framework combines a global approach (TCAV) with a local approach (LIME). To test the effectiveness of our approach, we conducted experiments on both the ImageNet and CelebA datasets. These experiments demonstrated that our framework can produce explanations that are easy for humans to understand. By providing local concept-based explanations, our framework has the potential to improve the transparency and interpretability of deep neural networks in a variety of applications.},
}
@inproceedings{shukla2017deep,
  title={A deep learning frame-work for recognizing developmental disorders},
  author={Shukla, Pushkar and Gupta, Tanu and Saini, Aradhya and Singh, Priyanka and Balasubramanian, Raman},
  booktitle={2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={705--714},
  year={2017},
  abstract={Developmental Disorders are chronic disabilities that have a severe impact on the day to day functioning of a large section of the human population. Recognizing developmental disorders from facial images is an important but a relatively unexplored challenge in the field of computer vision. This paper proposes a novel framework to detect developmental disorders from facial images. A spectrum of disorders constituting of Autism Spectrum Disorder, Cerebral Palsy, Fetal Alcohol Syndrome, Down syndrome, Intellectual disability and Progeria have been considered for recognition. The framework relies on Deep Convolutional Neural Networks (DCNN) for feature extraction. A new data-set comprising of images of subjects with these disabilities was built for testing the performance of the frame work. This model has been tested on different age groups, individual disabilities and has also been compared to a similar model that uses human intelligence to identify different developmental disorders. The results indicate that the model performs better than average human intelligence in terms of differentiating amongst different disabilities and is able to recognize subjects with these developmental disorders with an accuracy of 98.80%.},
}
@inproceedings{shukla2017deep,
  title={A deep learning frame-work for recognizing developmental disorders},
  author={Shukla, Pushkar and Gupta, Tanu and Saini, Aradhya and Singh, Priyanka and Balasubramanian, Raman},
  booktitle={2017 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={705--714},
  year={2017},
  organization={IEEE}
  abstract={Text-based games have emerged as an important test-bed for Reinforcement Learning (RL) research, requiring RL agents to combine grounded language understanding with sequential decision making. In this paper, we examine the problem of infusing RL agents with commonsense knowledge. Such knowledge would allow agents to efficiently act in the world by pruning out implausible actions, and to perform look-ahead planning to determine how current actions might affect future world states. We design a new text-based gaming environment called TextWorld Commonsense (TWC) for training and evaluating RL agents with a specific kind of commonsense knowledge about objects, their attributes, and affordances. We also introduce several baseline RL agents which track the sequential context and dynamically retrieve the relevant commonsense knowledge from ConceptNet. We show that agents which incorporate commonsense knowledge in TWC perform better, while acting more efficiently. We conduct user-studies to estimate human performance on TWC and show that there is ample room for future improvement.},
}
@article{shukla2019should,
  title={What should I ask? using conversationally informative rewards for goal-oriented visual dialog},
  author={Shukla, Pushkar and Elmadjian, Carlos and Sharan, Richika and Kulkarni, Vivek and Turk, Matthew and Wang, William Yang},
  journal={arXiv preprint arXiv:1907.12021},
  year={2019},
  abstract={The ability to engage in goal-oriented conversations has allowed humans to gain knowledge, reduce uncertainty, and perform tasks more efficiently. Artificial agents, however, are still far behind humans in having goal-driven conversations. In this work, we focus on the task of goal-oriented visual dialogue, aiming to automatically generate a series of questions about an image with a single objective. This task is challenging since these questions must not only be consistent with a strategy to achieve a goal, but also consider the contextual information in the image. We propose an end-to-end goal-oriented visual dialogue system, that combines reinforcement learning with regularized information gain. Unlike previous approaches that have been proposed for the task, our work is motivated by the Rational Speech Act framework, which models the process of human inquiry to reach a goal. We test the two versions of our model on the GuessWhat?! dataset, obtaining significant results that outperform the current state-of-the-art models in the task of generating questions to find an undisclosed object in an image.},
}
@inproceedings{elmadjian20183d,
  title={3D gaze estimation in the scene volume with a head-mounted eye tracker},
  author={Elmadjian, Carlos and Shukla, Pushkar and Tula, Antonio Diaz and Morimoto, Carlos H},
  booktitle={Proceedings of the Workshop on Communication by Gaze Interaction},
  pages={1--9},
  year={2018},
  abstract={Most applications involving gaze-based interaction are supported by estimation techniques that find a mapping between gaze data and corresponding targets on a 2D surface. However, in Virtual and Augmented Reality (AR) environments, interaction occurs mostly in a volumetric space, which poses a challenge to such techniques. Accurate point-of-regard (PoR) estimation, in particular, is of great importance to AR applications, since most known setups are prone to parallax error and target ambiguity. In this work, we expose the limitations of widely used techniques for PoR estimation in 3D and propose a new calibration procedure using an uncalibrated head-mounted binocular eye tracker coupled with an RGB-D camera to track 3D gaze within the scene volume. We conducted a study to evaluate our setup with real-world data using a geometric and an appearance-based method. Our results show that accurate estimation in this setting still is a challenge, though some gaze-based interaction techniques in 3D should be possible.}
}
@inproceedings{shukla2018automatic,
  title={Automatic cricket highlight generation using event-driven and excitement-based features},
  author={Shukla, Pushkar and Sadana, Hemant and Bansal, Apaar and Verma, Deepak and Elmadjian, Carlos and Raman, Balasubramanian and Turk, Matthew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={1800--1808},
  year={2018},
  abstract={Producing sports highlights is a labor-intensive work that requires some degree of specialization. We propose a model capable of automatically generating sports highlights with a focus on cricket. Cricket is a sport with a complex set of rules and is played for a longer time than most other sports. In this paper we propose a model that considers both event-based and excitement-based features to recognize and clip important events in a cricket match. Replays, audio intensity, player celebration, and playfield scenarios are examples of cues used to capture such events. To evaluate our framework, we conducted a set of experiments ranging from user studies to a comparison analysis between our highlights and the ones provided by the official broadcasters. The general approval by users and the significant overlap between both kinds of highlights indicate the usefulness of our model in real-life scenarios},
}
@inproceedings{shukla2017computer,
  title={A computer vision framework for detecting and preventing human-elephant collisions},
  author={Shukla, Pushkar and Dua, Isha and Raman, Balasubramanian and Mittal, Ankush},
  booktitle={Proceedings of the IEEE international conference on computer vision workshops},
  pages={2883--2890},
  year={2017},
  abstract={Human Elephant Collision (HEC) is a problem that is quite common across many parts of the world. There have been many incidents in the past where conflict between humans and elephants has caused serious damage and resulted in the loss of lives as well as property. The paper proposes a frame-work that relies on computer vision approaches for detecting and preventing HEC. The technique initially recognizes the areas of conflict where accidents are most likely to occur. This is followed by elephant detection system that identifies an elephant in the video frame. Two different algorithms to detect the presence of elephants having a mean average precision of 98.621% and 97.667% have been proposed in the paper. The position of the elephant once detected is tracked with respect to the area of conflict with a particle filter. A warning message is displayed as soon as the position of the elephant overlaps with the area of conflict. The results of the techniques that were applied on videos were discussed in the paper.}
}





